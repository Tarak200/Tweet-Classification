# -*- coding: utf-8 -*-
"""Tweet_classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kyJ1fuuOKFdGv8nur78Jy5GZPceAZrEO
"""

import re         # it is a library used from preprocessing text data
import numpy as np
from sklearn.datasets import load_files
import pickle    # converting data into bytes to store in files etc
import nltk
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize, sent_tokenize
import pandas as pd
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer   # here, it is using bag of words model
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.naive_bayes import GaussianNB 
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix

stops = list(stopwords.words('english'))     # to get the stopwords of desired language
stops.append('.')
print(stops)

data = "All work and no play makes jack dull boy. All work and no play makes puke a dull boy."
words = word_tokenize(data)     # word tokenizing 
words_flt = []

for w in words:
  if w not in stops:
    words_flt.append(w)

words_flt

X = pd.read_csv("Corona_NLP_train.csv", encoding="ISO-8859-1")
y = pd.read_csv("Corona_NLP_test.csv")
x_flt = X[["OriginalTweet", "Sentiment"]]
y_flt = y[["OriginalTweet", "Sentiment"]]

x_num = []
y_num = []

for i in range(len(x_flt)):
    if x_flt["Sentiment"][i] == "Neutral":
        x_num.append(0)
    elif x_flt["Sentiment"][i] == "Positive":
        x_num.append(1)
    elif x_flt["Sentiment"][i] == "Extremely Negative":
        x_num.append(-1)
    else:
        x_num.append(-0.5)


for i in range(len(y_flt)):
    if y_flt["Sentiment"][i] == "Neutral":
        y_num.append(0)
    elif y_flt["Sentiment"][i] == "Positive":
        y_num.append(1)
    elif y_flt["Sentiment"][i] == "Extremely Negative":
        y_num.append(-1)
    else:
        y_num.append(-0.5)


x_final = x_flt.assign(senti_num = x_num )
y_final = y_flt.assign(senti_num = y_num )
x_final.drop(['Sentiment'], axis=1)
y_final.drop(['Sentiment'], axis=1)
y_senti = x_final["senti_num"]

x_text = x_final["OriginalTweet"].to_list() # pandas donot have the attribute to directly convert to array, so we converted it to list
len(x_text)

documents = []
stemmer = WordNetLemmatizer()

for sen in range(0,len(x_text)):
  # Remove all the special characters
    document = re.sub(r'\W', ' ', str(x_text[sen]))
        
    # remove all single characters
    document = re.sub(r'\s+[a-zA-Z]\s+', ' ', document)
    
    # Remove single characters from the start
    document = re.sub(r'\^[a-zA-Z]\s+', ' ', document) 
    
    # Substituting multiple spaces with single space
    document = re.sub(r'\s+', ' ', document, flags=re.I)
    
    # Removing prefixed 'b'
    document = re.sub(r'^b\s+', '', document)
    
    # Converting to Lowercase
    document = document.lower()
    
    # Lemmatization is the process of converting a word into its baseform
    document = document.split()

    document = [stemmer.lemmatize(word) for word in document]
    document = ' '.join(document)    #joins different strings into a single string
    documents.append(document)

#print(documents)

doc_flt = []

for w in documents:
  if w not in stops:
    doc_flt.append(w)

doc_flt_arr = np.array(doc_flt)

# converting text to numbers using bag of words model
# the bag of words model and the word embedding model are two approaches used
# genearlly bag of words works better than word embeddings


#here, this bag of words model takes the max 1500 unique words which has min prescence in 5 docs and maximum prescence of 70% documents (if they present in all docs, they dont give any contextual meaning) with stop words of english
#bag of words model take all the unique words and represent a sentence a in the form of unique words

vectorizer = CountVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))
X_BoW = vectorizer.fit_transform(documents).toarray()

#converting sentences into word embeddings 
# every word in the document is represented as a product of term frequency which is ratio of no. of occurences of that word in the sentence to the whole occurences in the document and the inverse document frequency which measure the rarity of the word in th whole document
tfidfconverter = TfidfVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))
X_tfidf = tfidfconverter.fit_transform(documents).toarray()
print(X)

X_train, X_test, y_train, y_test = train_test_split(X_BoW,y_senti, test_size=0.2, random_state=42)
DTC  = DecisionTreeRegressor()
DTC.fit(X_train, y_train)
y_pred = DTC.predict(X_test)
y_pred